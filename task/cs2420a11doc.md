# Assignment 11

This project was done by I (Chase Yates) and Liam Astin. I turned in the assignment on Gradescope.

## Question 1

### Part 1

I parsed the input file through a few different methods. To get rid of comments, I constructed a list of strings which started on lines with curly braces and ended on the lines with closed curly brackets, thus each `String` represented the text of a non-terminal definition. This was done inside a `GrammarBuilder` class, which would eventually be used to generate phrases with its `GeneratePhrase` method which is random every time. With the definition strings, I pass them to a `GrammarRule` class which represents the definition of a non-terminal. I passed the strings to the `GrammarRule` constructor, which took the text and parsed it. To do this, I split the text into a stream of lines and iterated through it, ignoring the first and last lines. I then use a method to remove the angle brackets and extract the name of the non-terminal, and set the `name` field in the `GrammarRule` instance. I then take each of the remaining lines of text provided, and pass those into the constructor of the `Production` class to create an `ArrayList` of production rules as a field in `GrammarRule`. The Production class separates the non-terminals from terminals, and constructs a list of `Content` instances, which represent either terminal text, a non-terminal, or a non-terminal with circular dependency. The non-terminals reference a Java `Hashtable` within the `GrammarBuilder` called `rules` to get references to the definitions of non-terminals so it can evaluate them on the fly. When constructing `Content`, it is passed a source `GrammarRule` and if it is the same as the `GrammarRule` it references, then it is a circular dependency. It is marked as `referTo` with a boolean and only stores the text of the non-terminal reference, lazily evaluating it when called as to not cause a stack overflow. When evaluating a `GrammarRule` it uses a random number generator to get an index within its list of `Production`s and it evaluates that when called.

### Part 2

We used a few different data structures when constructing the solution to this assignment. As mentioned in [[#Part 1]], we used a Java `Hashtable` for holding and accessing the non-terminal definitions in constant time as to reduce overhead when defining the data structures for the grammar. I considered writing my own `Hashtable` implementation but realized that using Java's would be quicker and more robust, serving the same role in the end. The `GrammarRule`s use a Java `ArrayList` to hold their production rules in a simple list format for constant time data access, meanwhile a `Production` holds an `ArrayList` of `Content`. The overall structure of the Grammar is, although not explicitly, a kind of dependency graph, as the `GrammarBuilder` relies on a `Hashtable` of many `GrammarRule`s, which contain a list of `Production`s, which hold a list of `Content`, which each either end their (are terminal), or reference another `GrammarRule`, and the dependencies continue from their. To avoid the rule of circular dependencies, if a `Content` instance references a `GrammarRule` it relies on, it will only store the text representing the `GrammarRule` and lazily figure out what it represents when requested. Any classes which needed to be evaluated inherited from a common interface called `Expression` for organization purposes, containing one method called `evaluate` which returns a `String`. I didn't strongly consider any other data structures.

### Part 3

The Java classes I used were `Exception`, `IOException`, `FileNotFoundException`, `Hashtable`, `ArrayList`, `String`, `Integer`, and `System`.

My classes were `RandomPhraseGenerator` (which we were told to create as part of the assignment), `GrammarBuilder`, `GrammarRule`, `Production`, and `Content`. I also had an enum named `ContentType` and an interface named `Expression`.

## Question 2

### Experiment 1

In this experiment, I used the provided `assignment_extension_request.g` grammar file, and generated N random phrases. I went from N=1000 to N=20000, looping 100 times

![[a11c1.png]]

### Experiment 2

In this experiment I hand wrote a range of grammar files, all only containing the start non-terminal and another non-terminal named 'a' with N production rules, all containing the same terminal sentence. I started with N=1 to N=10 looping 10,000 times.

![[a11c2.png]]

## Question 3

For both experiments, I expected the running times to end up being O(N). For the first experiment, I expected an O(N) time as the grammar data structure is built ahead of time and the production of phrases (N) uses the same set of steps on average, the only thing growing is the number of phrases to generate, linearly based on N. For the second experiment, I considered most of the grammar structure build time to be constant as the files were identical except the number of productions in the second non-terminal definition (N), which should take the same amount of time each, resulting in another O(N) time.  The charts shown closely reflect these predictions, with a linear shape on average.

## Question 4

There are always other ways to solve a problem. My solution may be fast, although I don't think it is the fastest. I'm sure another one of my peers came up with a solution that is faster in certain circumstances. The way I designed my program, doing most of the work at the start constructing the grammar data structure, then quickly accessing parts of it later for random phrase generation, is especially good when constructing large grammar structures and generating many phrases, as the brunt of the work is done efficiently and robustly at the start, then quickly referred to later. I bet other solutions do a less robust job of interpreting the grammar files and simply come up with the solutions to random generation on the fly, resulting in faster speeds with simple grammar files and few phrases to generate, as the time it takes to generate a phrase is slower overall, re-evaluating the grammar provided. Different solutions have different runtimes based on a variety of factors, so even using the terms 'fastest' or 'most efficent' is - to an extent - a red herring.

## Question 5

I think our solution is well designed and well written. Different concepts have different classes associated and use pretty simple solutions for parsing the text. The way of representing the grammar in my opinion might be a bit over-coupled, but makes a lot of sense and is very efficient. I can run the program generating 10,000 phrases on my computer on the command line and it is done in the snap of a finger. I think the `Expression` interface is unneeded but represents a nice way of showing that the different classes have a similar way of propagating their information up the tree when `GeneratePhrase` and `evaluate` are called. The code is well commented and organized in my opinion. If I were to write it again it would be somewhat different of course, but the same ideas and ways of generating structures and organizing information would likely be the same.